% matrizdensidad.tex
%
% Copyright (C) 2020-2025 José A. Navarro Ramón <janr.devel@gmail.com>
% 1) Código fuente:
% Licencia GNU-2
%
% 2) Texto legible en cualquier formato: pdf, postscript, html, etc.:
% Licencia Creative Commons Recognition-NonCommercial-ShareAlike.
% (CC-BY-NC-SA)
% ----------------------------------------------------------------------------

\chapter{Matriz Densidad}

\section{Introducción}
Por comodidad, utilizaremos un espacio de Hilbert $\left(\mathbb{C}^2\right)$,
de dimensión 2

\[
  \mathlarger{\mathcal{H} = \mathbb{C}^2}
\]

Ahora, elegiremos los símbolos para una base de este espacio. Para ello, se
podrían elegir algunos símbolos como $\ket{\varphi_1}$ y $\ket{\varphi_2}$,
$\ket{e_1}$ y $\ket{e_2}$, o $\ket{a}$ y $\ket{b}$, entre otros. En el fondo,
son solo símbolos, sin más, Pero vamos a introducir una nomenclatura que es
propia de la información cuántica y que hace referencia al \emph{qbit}.

Así, representaremos la base\footnotemark{} como
\footnotetext{La base está formada por dos vectores en un espacio de Hilbert
  de dimensión 2; en general tendría $n$ vectores.}
\[
  B = \{\ket{0}, \ket{1}\}
\]

El cero y el uno son solo etiquetas. No hay nada profundo en esto, solo
recuerdan al bit clásico. Únicamente añadir, por completitud, que por
\emph{qbit} se entiende aquel vector que es una combinación lineal de estos
dos vectores de la base
\[
  \ket{\Psi} = a \ket{0} + b \ket{1} ;
  \hspace{2em} a, b \in \mathbb{C}
\]
aunque no vamos a entrar en esto, \dots dejaremos los \emph{qbits} a un lado.

A partir de ahora solo nos interesa el  espacio de Hilbert de dimensión 2
y dos vectores de la base.
Como es costumbre cuando se trabaja en un espacio de dimensión finita, podemos
utilizar una expresión matricial. Así, representaremos los vectores de la base
como
\[
  \ket{0} = \begin{pmatrix}1 \\ 0\end{pmatrix};
  \hspace{0.5em}
  \ket{1} = \begin{pmatrix}0 \\ 1\end{pmatrix}
\]
y sus duales
\[
  \bra{0} = \ket{0}^\dagger
  = \begin{pmatrix}1 \\ 0\end{pmatrix}^\dagger
  = \begin{pmatrix}1 & 0\end{pmatrix}
  \hspace{4em}
  \bra{1} = \ket{1}^\dagger
  = \begin{pmatrix}0 \\ 1\end{pmatrix}^\dagger
  = \begin{pmatrix}0 & 1\end{pmatrix}
\]

\subsection{Operadores autoadjuntos}
En mecánica cuántica, un observable está asociado a un operador cuyo adjunto
coincide con él mismo; así se garantiza que sus valores propios serían reales.
Un operador así se denomina \emph{autoadjunto}
\[
  \hat{A}^\dagger = \hat{A}
\]

Para cualquier operador autoadjunto $\hat{A}$, se puede encontrar una nueva
base, que es propia del operador
\[
  B_{\hat{A}}= \{\ket{a_1}, \ket{a_2}\}
\]
que, por ser base, es ortogonal y, por ser autoadjunto, sus valores propios son
reales
\begin{align*}
  &\hat{A} \ket{a_1} = \lambda_1 \ket{a_1}\\
  &\hat{A} \ket{a_2} = \lambda_2 \ket{a_2}
\end{align*}
donde $\lambda_{1}, \lambda_{2} \in \mathbb{R}$.
En otras palabras, la representación matricial del operador en su base propia
es una matriz diagonal
\[
  \mmm{\hat{A}}
  \equiv
  \begin{pmatrix}
    \lambda_1 & 0 \\
    0 & \lambda_2
  \end{pmatrix}
\]
Recordemos que estamos en un espacio de dimensión 2, aunque esto también es
válido, ajustando la dimensión de la matriz,  para un espacio finito de
dimensión superior.
A menos que se diga lo contrario, los operadores que nos encontremos serán
autoadjuntos.

\subsection{Valor esperado de un operador}
Supongamos que un sistema está descrito por el estado $\ket{\Psi}$, y nos
preguntamos acerca del valor esperado de un cierto operador $\hat{A}$, en  ese
estado.

Si estamos en un mundo de dimensión 2, y la base que utilizamos es
$\{\ket{0}, \ket{1}\}$, el estado se puede expresar como una combinación lineal
de los vectores de la base
\[
  \ket{\Psi} = a_0 \ket{0} + a_1 \ket{1}
  \text{, donde }
  a_1, a_2 \in \mathbb{C}
\]

El \emph{valor esperado del observable} $\hat{A}$, cuando el sistema está en el
estado $\ket{\Psi}$, se expresa mediante el producto escalar
\begin{align*}
  \braket{\hat{A}}_{\Psi}
  &= \braket{\Psi | \hat{A} | \Psi}
  = \left(a_{0}^{*} \bra{0} + a_{1}^{*} \bra{1}\right)
  \hat{A}
    \left(a_{0} \ket{0} + a_{1} \ket{1}\right)\\
  &= a_{0}^{*} a_{0} \braket{0|\hat{A}|0}
    + a_{0}^{*} a_{1} \braket{0|\hat{A}|1}
    + a_{1}^{*} a_{0} \braket{1|\hat{A}|0}
    + a_{1}^{*} a_{1} \braket{1|\hat{A}|1}\\
  &= a_{0} a_{0}^{*} \braket{0|\hat{A}|0}
    + a_{1} a_{0}^{*} \braket{0|\hat{A}|1}
    + a_{0} a_{1}^{*} \braket{1|\hat{A}|0}
    + a_{1} a_{1}^{*} \braket{1|\hat{A}|1}\\
  &= a_{0} a_{0}^{*} A_{00}
    + a_{1} a_{0}^{*} A_{01}
    + a_{0} a_{1}^{*} A_{10}
    + a_{1} a_{1}^{*} A_{11}
\end{align*}
donde hemos utilizado $A_{ij} = \braket{i|\hat{A}|j}$.

Si ahora llamamos $\rho_{ij} = a_{i} a_{j}^{*}$, obtenemos
\begin{equation}
  \braket{\hat{A}}_{\Psi}
  = \braket{\Psi | \hat{A} | \Psi}
  = \rho_{00} A_{00}
    + \rho_{10} A_{01}
    + \rho_{01} A_{10}
    + \rho_{11} A_{11}  
\end{equation}
Obsérvese que el valor esperado de un operador autoadjunto es un número real.

Las $\rho_{ij}$ las podríamos interpretar como elementos de una matriz
$2\times 2$. Por otro lado, sabemos por mecánica cuántica que las $A_{ij}$ son
elementos de una matriz. Así que
\[
  \mmm{\rho}
  = \begin{pmatrix}\rho_{00} & \rho_{01} \\ \rho_{10} & \rho_{11} \end{pmatrix}
  ;\hspace{4em}
  \mmm{A}
  = \begin{pmatrix}A_{00} & A_{01} \\ A_{10} & A_{11} \end{pmatrix}
\]

Pero, al multiplicar estas dos matrices $2\times 2$ obtendríamos otra matriz
$2\times 2$, no un número. Pero, observamos que el valor esperado coincide 
con la suma de los elementos diagonales
\[
  \mmm{\rho} \mmm{A}
  = \begin{pmatrix}\rho_{00} & \rho_{01} \\ \rho_{10} & \rho_{11} \end{pmatrix}
  \begin{pmatrix}A_{00} & A_{01} \\ A_{10} & A_{11} \end{pmatrix}
  = \begin{pmatrix}
    \rho_{00} A_{00} + \rho_{01} A_{10} & \cdots \\
    \cdots & \rho_{10} A_{01} + \rho_{11} A_{11}
  \end{pmatrix}
\]

La suma de los elementos diagonales de una matriz cuadrada es su \emph{traza}
\[
  \braket{\hat{A}}_{\Psi}
  = \braket{\Psi|\hat{A}|\Psi}
  = \text{Tr}(\mmm{\rho}\mmm{\hat{A}})
\]

¿Por qué nos complicamos tanto para calcular el valor esperado de un observable
en el estado $\ket{\Psi}$, si lo podemos calcular directamente como
$\braket{\Psi|\hat{A}|\Psi}$?

Bueno, nos complicamos algo, pero en ocasiones es importante hacerlo. Resulta
que en estadística, o incluso, en teoría de información cuántica, podría
ocurrir que nos dijeran que la probabilidad de que un sistema se encontrara en
un estado, digamos, $\ket{\alpha}$, fuera $1/3$ y la de encontrarse en
$\ket{\beta}$ fuera $2/3$. Es decir, que no nos aseguren que el sistema se
encuentre en un estado cuántico concreto.

Este tipo de situación introduce una incertidumbre clásica, pues estas
probabilidades son absolutamente clásicas. Además, en los estados cuánticos
$\ket{\alpha}$ y $\ket{\beta}$, también hay probabilidades cuánticas.
Si ocurriera esto nos complicaría bastante la vida, y su desarrollo sería muy
laborioso, muy difuso y muy poco claro si utilizáramos la nomenclatura
estándar del cálculo vectorial de toda la vida, y resulta que introduciendo
esta matriz densidad $\mmm{\rho}$, todo es mucho más fácil e intuitivo.

Podemos observar que la matriz densidad depende solo de los coeficientes de la
función de onda en nuestra base, $\ket{\Psi} = a_0 \ket{0} + a_1 \ket{1}$
\[
  \mmm{\rho}
  = \begin{pmatrix}\rho_{00} & \rho_{01}\\ \rho_{10} & \rho_{11}\end{pmatrix}
  = \begin{pmatrix}
    a_{0} a_{0}^{*} & a_{0} a_{1}^{*} \\
    a_{1} a_{0}^{*} & a_{1} a_{1}^{*}
    \end{pmatrix}
\]
así que está relacionada directamente con el estado $\ket{\Psi}$.

\subsection{Estado puro}
Ahora mostraremos otra forma de obtener la matriz densidad, multiplicando el
estado por su dual, en este orden\footnotemark{}
\footnotetext{Este producto no es escalar. Se conoce como producto tensorial.
  Ya hablaremos más adelante de él.}
\[
  \ket{\Psi}\bra{\Psi}
  = \begin{pmatrix}a_0 \\ a_1\end{pmatrix}
  = \begin{pmatrix}a_{0}^{*} & a_{1}^{*}\end{pmatrix}
  = \begin{pmatrix}
    a_{0} a_{0}^{*} & a_{0} a_{1}^{*} \\
    a_{1} a_{0}^{*} & a_{1} a_{1}^{*}
  \end{pmatrix}
  = \mmm{\rho}
\]
Este resultado se obtiene cuando no tenemos ninguna duda de cuál es el estado
del sistema ---\emph{estado puro}---. Si el estado cuántico se conociera con una
cierta incertidumbre clásica ---\emph{estado mixto}---, no se habría obtenido
este resultado.

Diremos pues, que un sistema se encuentra en un \emph{estado cuántico puro} si
\begin{equation}
  \mmm{\rho} = \ket{\Psi} \bra{\Psi}
\end{equation}

\section{Propiedades de la matriz densidad}
\begin{enumerate}
\item {\bfseries $\mmm{\rho}$ es un operador autoadjunto\footnotemark{}.}
  \footnotetext{El que sea autoadjunto no implica, necesariamente, que
    represente a un observable físico. La proposición inversa sí es cierta.}
  Recordemos que un operador $\mmm{\hat{A}}$ es autoadjunto si su adjunto
  coincide con él mismo. Vamos a demostrar que la matriz densidad tiene esta
  propiedad
  \begin{align*}
    \mmm{\rho}^\dagger
    &= \begin{pmatrix}
      a_0 a_0^* & a_0 a_1^* \\
      a_1 a_0^* & a_1 a_1^*
    \end{pmatrix}^\dagger
    = \begin{pmatrix}
      (a_0 a_0^*)^* & (a_1 a_0^*)^* \\
      (a_0 a_1^*)^* & (a_1 a_1^*)^*
    \end{pmatrix}\\
    &= \begin{pmatrix}
      a_0^* a_0 & a_1^* a_0 \\
      a_0^* a_1 & a_1^* a_1
    \end{pmatrix}
    = \begin{pmatrix}
      a_0 a_0^* & a_0 a_1^* \\
      a_1 a_0^* & a_1 a_1^*
    \end{pmatrix}
    = \mmm{\rho}
  \end{align*}

  Por tanto, la matriz densidad representa a un operador autoadjunto.
  A partir de ahora representaremos de manera apropiada a este operador,
  con el acento circunflejo, $\mmm{\hat{\rho}}$. Este operador describe el
  estado de un sistema cuántico.
  
\item {\bfseries La traza de la matriz densidad es la unidad.}
  La traza de una matriz cuadrada es la suma de sus elementos diagonales.
  \[
    \mmm{\hat{\rho}}
    = \begin{pmatrix}
      a_0 a_0^* & a_0 a_1^* \\
      a_1 a_0^* & a_1 a_1^*
    \end{pmatrix}
  \]

  \[
    \text{Tr}(\mmm{\hat\rho})
    = a_0 a_0^* + a_1 a_1^*
    = |a_0|^2 + |a_1|^2 = 1
  \]

  Como $a_0$ y $a_1$ son las componentes del vector de estado del sistema, que
  está normalizado, la suma de los cuadrados de sus módulos debe valer la
  unidad.
  \[
    \ket{\Psi} = a_0 \ket{0} + a_1 \ket{1}
  \]

\item {\bfseries El operador $\mmm{\hat{\rho}}$ es positivo
    ($\mmm{\hat{\rho}} > 0$).}
  Esto quiere decir que para cualquier ket $\ket{u}$ en nuestro espacio de
  Hilbert, se debe cumplir
  \[
    \braket{u|\mmm{\hat{\rho}}|u} > 0
  \]

  Otra manera de decirlo es que, dado que $\mmm{\hat{\rho}}$ es autoadjunto, se
  puede diagonalizar ---expresarse en su base propia---
  \[
    B_u = \set{\ket{u_1}, \ket{u_2}}
  \]
  y los valores propios, aquí $\lambda_1$ y $\lambda_2$, deben ser no negativos
  \begin{align*}
    \hat{\rho} \ket{u_1} &= \lambda_1 \ket{u_1}\\
    \hat{\rho} \ket{u_2} &= \lambda_2 \ket{u_2}
  \end{align*}
  El operador densidad diagonalizado tendría esta forma
  \[
    \mmm{\hat{\rho}}
    = \begin{pmatrix}\lambda_1 & 0 \\ 0 & \lambda_2\end{pmatrix}
    \hspace{2em}\text{con } \lambda_1, \lambda_2 \geq 0    
  \]
  
  Obsérvese que, aunque los valores propios han de ser no negativos, no pueden
  ser ambos cero, pues la traza de la matriz debe valer la unidad.
\end{enumerate}

Como resumen, recordamos que para que una matriz $\mmm{\hat{\rho}}$ pueda ser
considerada matriz densidad, debe tener las tres propiedades que presentamos de
forma simbólica
\begin{enumerate}
\item $\mmm{\hat{\rho}}^\dagger = \mmm{\hat{\rho}}$
\item $\text{Tr}(\mmm{\hat\rho}) = 1$
\item $\mmm{\hat{\rho}} > 0$
\end{enumerate}

\subsection{Combinación lineal de matrices densidad}
Ahora nos interesa saber si una combinación lineal de matrices densidad, como
\[
  \mmm{\hat{\rho}} = \alpha \mmm{\hat{\rho}}_1 + \beta \mmm{\hat{\rho}}_2
\]
es también una matriz densidad.
Para que esto sea así, la combinación lineal debe cumplir las tres propiedades
anteriores.

\begin{description}
\item[Matriz definida positiva:] Como $\mmm{\hat{\rho}}_1$ y
  $\mmm{\hat{\rho}}_2$ son matrices densidad, ambas son positivas
  \[
    \mmm{\hat{\rho}}_1 > 0
    \hspace{1em}y\hspace{1em}
    \mmm{\hat{\rho}}_2 > 0
  \]
  entonces, podremos decir, sin demostrar, que si los coeficientes $\alpha$ y
  $\beta$ son números reales no negativos (ambos no nulos)
  \[
    \alpha, \beta \in \mathbb{R}^{+} \cup \set{0}
  \]
  entonces la combinación lineal $\mmm{\hat{\rho}}$, también sería definida
  positiva.
  
\item[Matriz ajunta:]
  También podríamos demostrar que si $\mmm{\hat{\rho}}_1$ y
  $\mmm{\hat{\rho}}_2$ son operadores autoadjuntos y $\alpha$ y $\beta$ son
  números reales, entonces, $\mmm{\hat{\rho}}$ también es autoadjunta.
  
\item[Matriz con traza unidad]
  La traza de una matriz es una operación lineal, y por tanto
  \[
    \text{Tr}{\mmm{\hat\rho}}
    = \text{Tr}(\alpha \mmm{\hat{\rho}}_1 + \beta \mmm{\hat{\rho}}_2)
    = \alpha\,\text{Tr}(\mmm{\hat{\rho}}_1)
    + \beta\,\text{Tr}(\mmm{\hat{\rho}}_2)
    = \alpha \cdot 1 + \beta \cdot 1
    = \alpha + \beta
  \]
  Luego, para que se cumpla $\text{Tr}\mmm{\hat{\rho}} = 1$, la suma de los
  coeficientes debe valer la unidad
  \[
    \alpha + \beta = 1
  \]
\end{description}

Hemos llegado a la conclusión de que si construyo una nueva matriz densidad
mediante una combinación lineal de otras, obtendré una matriz densidad, con
tal de que los coeficientes sean reales no negativos y sumen la unidad
\[
  \mmm{\hat{\rho}} = \sum_i P_i \mmm{\hat{\rho}}_i
  \text{,}\hspace{1em} \text{con} P_i \geq 0
  \hspace{1em}\text{ y }\hspace{1em} \sum_i P_i = 1
\]

Esta propiedad está relacionada con una distribución de probabilidad discreta
clásica.

Ahora, volvemos al principio de nuestra discusión. Supongamos que tenemos un
sistema cuántico, que puede estar descrito por los estados $\ket{\Psi_1}$, y
$\ket{\Psi_2}$, que pueden ser cualesquiera, no tienen que ser ortogonales ni
nada. Además, supongamos que nos dicen que la probabilidad de que el sistema
esté descrito por $\ket{\Psi_1}$ es, por ejemplo, de $1/4$ y, por tanto, la
probabilidad de que estuviera descrito por $\ket{\Psi_2}$ sería de $3/4$

\begin{figure}[ht]
  % Escala
  \def\scl{1}
  % Eje x
  \pgfmathsetmacro{\XMLONG}{0}
  \pgfmathsetmacro{\XPLONG}{3}
  % Eje y
  \pgfmathsetmacro{\YMLONG}{0}
  \pgfmathsetmacro{\YPLONG}{3}
  % Ángulo rotado
  \pgfmathsetmacro{\ANGROT}{20}
  % Vector P
  \pgfmathsetmacro{\PMOD}{2.5}
  \pgfmathsetmacro{\PANG}{60}
  % Vector P'
  \pgfmathsetmacro{\PPRIMAMOD}{\PMOD}
  \pgfmathsetmacro{\PPRIMAANG}{\PANG - \ANGROT}
  % 
  \centering
  \begin{tikzpicture}[%
    scale=\scl,
    every node/.style={black,font=\small},
    eje/.style={->},
    vector/.style={-{Latex}, shorten >=1.2pt, line width=.8pt},
    vectorrotado/.style={vector, draw=green!50!black},
    pcirculo/.style={fill=red, draw=black},
    pprimacirculo/.style={green!90!black, draw=black},
    background/.style={
      line width=\bgborderwidth,
      draw=\bgbordercolor,
      fill=\bgcolor,
    },
    ]
    % Coordenadas
    \coordinate (O) at (0,0);
    \node[cloud, draw, fill=gray!20, aspect=2]
    {SISTEMA\hspace{0.2em} $\ket{?}$};
    %\begin{scope}[on background layer]
    %  \node [background, fit= (incx) (incy) (letraejex) (letraejey)] {};
    %\end{scope}
  \end{tikzpicture}
  \caption{Sistema cuántico afectado por probabilidades clásicas.}
  %\label{fig:gli-girovector}
\end{figure}

Supongamos que nos dicen que la probabilidad de que el sistema esté descrito
por $\ket{\Psi_1}$ es, por ejemplo, $1/4$, y de que lo esté por $\ket{\Psi_2}$
es $3/4$.
¿Cómo podríamos describir este sistema? La forma más útil no es con un
\emph{ket} ---que no sabemos cuál es---. Lo mejor es hacerlo a través de la
matriz densidad que va a ser
\[
  \mmm{\rho} = \dfrac{1}{4}\,\mmm{\rho_1} + \dfrac{3}{4}\,\mmm{\rho_2}
\]
donde $\mmm{\rho_1} = \ket{\Psi_1}\bra{\Psi_1}$ y
$\mmm{\rho_2} = \ket{\Psi_2}\bra{\Psi_2}$.

Cuando hay probabilidades clásicas en el sistema, es mejor describirlo mediante
la matriz densidad.
Veamos ejemplos para probar en qué sentido es correcto hacerlo así.









%%% Local Variables:
%%% mode: latex
%%% TeX-engine: luatex
%%% TeX-master: "../retazosfisica.tex"
%%% End:
